\documentclass{acm_proc_article-sp}[12pt]
\usepackage{cite}
\usepackage[english]{babel}

\title{Massivelly Parallel Graphs: A Genome Assembly Use Case}
\numberofauthors{1}
\author{
% 1st. author
\alignauthor 
Lauro Beltr\~ao Costa, Abdullah Gharaibeh, Elizeu
Santos-Neto\vspace{3mm}\\
       \affaddr{\small{University of British Columbia}}\\
       \affaddr{\small{2332 Main Mall}}\\
       \affaddr{\small{Vancouver, BC, CANADA}}\\
       \email{\small{\{lauroc,abdullah,elizeus\}@ece.ubc.ca}}
}
\date{27 January 2011}

\begin{document}

\maketitle
\begin{abstract}
The goal of this project is to understand the challenges in porting
graph algorithms to hybrid parallel architectures (systems that
include both processors optimized for sequential processing and
accelerators optimized for massively-parallel processing). The
intention is to build a generic graph processing framework on such
architectures. 

To inform the design of the framework, we study a number of graph
algorithms that are in the core of many relevant applications. One
case study this project focuses on is the construction of De Brujin
graphs and the computation of Eulerian tours in such graphs on
GPU-based platforms (which integrates commodity processors (CPUs) and
Graphics Processing Units (GPUs)). De Brujin graphs are used to model
one of the most important bioinformatics procedures named genome
assembly, and computing the Eulerian tour is an essential phase in
this process.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
%\terms{Theory}
%\keywords{ACM proceedings, \LaTeX, text tagging} % NOT required for Proceedings

\section{Introduction}

Graphs are everywhere. From the nowadays omnipresent online social
networking tools to the structure of fundamental scientific problems,
passing through mundane applications such as finding the best car
route between two points in a city, graphs are indeed the building
block to model and approach several important problems.

Although graphs provide powerful abstractions to approach complex
problems, the scale of current graphs call for new computational
platforms and techniques that can lead to efficient processing.
Previous work investigates the use of massively-parallel
processors, particularly GPUs, to accelerate graph
computation~\cite{Harish2007, Katz2008, Malewicz2009, Sungpack2010,
  dehne2010exploring}. Yet the general challenges of porting
algorithms for large-scale graph processing to GPU-based platforms
(which are turning into mainstream), especially large multi-GPU
systems, are largely unknown. To this end, this work aims to address
the following questions:

\begin{enumerate}
  \item What are the general challenges in porting algorithms for
    large scale graph processing to hybrid, GPU-based systems? What is
    the performance of graph algorithms compared to the theoretical
    peak performance numbers of such systems? \\

  \item If we are to build a GPU-based graph processing framework to
    simplify the task of implementing graph algorithms on such
    platforms, what should this framework provide to the developer?
    Specifically, what would be the parallel processing model (e.g.,
    synchronous bulk processing, PRAM or LogP), the abstractions
    (e.g., vertex or edge centric) and the high-level interface of a
    sufficiently expressive, yet performance-efficient framework? 
  
  \item Since the goal of the above mentioned framework is to hide the
    complexity of programming graph-processing algorithms on GPU-based
    platforms, the framework may mask some hardware details that could
    have been used to better optimize specific graph algorithms. To
    this end, what would be the performance cost, if any, for
    implementing graph algorithms using such a framework? 
\end{enumerate}

To address these questions, this work follows a top-down approach: it
studies a number of high-impact graph algorithms and graph-based use
cases to inform the design of the framework. One such use case is
genome assembly.

The next sections present the use case application
(Section~\ref{sec:usecase}), the anticipated opportunities, goals and
challenges of this project (Section~\ref{sec:opp}), related work
(Section~\ref{sec:related}), and the intended methodology
(Section~\ref{sec:methodology}).

\section{Genome Sequence Assembly}
\label{sec:usecase}
One important application this work focuses on is genome sequence
assembly: a high-impact, graph-based bioinformatics problem that aims
to reconstruct a genome sequence from a large number of short sequence
fragments, called reads. The genome assembly use case involves the
construction (from the short reads) of a large-scale De Bruijn graph
and computing Eulerian Tours~\cite{pevzner2001eulerian,
  zerbino2008velvet} in such graph.

More formally, let $V={v_1,...,v_n}$ be the set of reads (i.e., short
sequences of characters from the DNA alphabet \{A, C, G, T\}). This
work targets a twofold graph problem:

\begin{enumerate}
 \item to construct, in parallel, a directed graph $G=(V,E)$, where
   the reads form the set of nodes $V$, and $(v_i,v_j) \in E$ is a
   directed edge if the last $c$ characters of $v_i$ overlap with the
   first $c$ characters of $v_j$.

 \item to compute, in parallel, an Eulerian tour over $G$. The tour is
   represented by a list of edges $T=<e_1,...,e_m>$ such that each edge 
   $e_k \in E$ appears in $T$ at most once. 
\end{enumerate}

\section{Opportunities, Challenges and Goals}
\label{sec:opp}

This project is motivated by the fact that current GPU models offer
significant peak performance advantages (computation and internal
memory bandwidth) compared to traditional multiprocessors. 

Moreover, large-scale graphs are data intensive in nature: graph
applications in domains such as social networks, web analysis and
bioinformatics process billions of vertices, which suggests an
opportunity for high data parallelism. Therefore, these
characteristics make such applications suitable for the new
generation of massively-parallel computing architectures such as
hybrid GPU-based platforms.

For example, the genome assembly use case is a data intensive problem
that processes billions of short reads. The application involves
constructing a De Bruijn Graph and computing an Eulerian
Tour~\cite{simpson2009abyss}, both of which have the potential to be
processed in parallel~\cite{Cormen2009,Atallah1984}. 

To realize such opportunities, however, it is necessary to overcome some 
challenges. First, the large amount of data required by large-scale
graphs puts great pressure on two scarce GPU resources: on-board
memory and host-device I/O bandwidth. Second, graph problems generally
exhibit irregular parallelism~\cite{Kulkarni2009}, and have little
locality on memory access patterns (e.g., vertex neighbours may be
scattered in memory). Finally, communication between compute resources
in a heterogeneous multi-GPU system may lead to performance
bottlenecks due to the irregular parallelism. These characteristics
makes it challenging to exploit GPU architectures, which have strict
parallel compute model (i.e., Single Instruction Multiple Data), and
rely on regular memory access patterns to achieve good performance.

Considering the opportunities and specific challenges above, the
specific goals of this project is threefold: 

\begin{enumerate}
  \item To build an initial design of a graph computing framework 
    for GPU-based platforms. The framework aims to provide an infrastructure
    to simplify the task of implementing parallel graph algorithms for
    large-scale GPU-based systems (e.g., multi-GPU systems and GPU
    clusters).

  \item To understand the challenges of porting graph algorithms to
    GPUs. One particular use case this work will focus on is computing 
    the Eulerian tour in De Bruijn Graphs.

  \item To implement and evaluate the use case application (i.e.,
    genome assembly).
\end{enumerate}

\section{Related Work}
\label{sec:related}
This project spans at least the following areas: graph algorithms, in
general, and parallel algorithms, in particular; parallel graph
processing frameworks; and genome assembly algorithms. This section
briefly comments on the related literature collected so far.

{\bf Parallel graph algorithms.} Although graph algorithms is a well
studied area, leveraging parallel architecture to accelerate the
algorithm runtime is not straightforward. A parallel implementation
may require changes to the original
algorithm~\cite{Quinn1984,Atallah1984} and the assumptions about the
parallel platform. The required changes come at the expense of making
the algorithm less portable.

Most work on parallel graph algorithms assume a PRAM parallel
processing model. For example, Atallah~\cite{Atallah1984} proposes an
algorithm for finding Eulerian tours in parallel. The algorithm
assumes a shared memory model, and works by partitioning the graph
into a collection of disjoint tours which are then merged together
into a single Euler tour.

{\bf Graph processing frameworks.} The Parallel
BGL~\cite{gregor2005parallel} and Pregel~\cite{Malewicz2009} present
frameworks to implement distributed graph algorithms. Both frameworks
specify generic concepts for designing distributed graph
algorithms. The frameworks are based on abstractions that are common
among graph algorithms. Examples of such abstractions include
vertices, edges, property maps {\em (key-value pairs)}, and mechanisms
for data propagation and graph traversal. 

Both PBGL and Pregel assume traditional cluster systems (i.e., nodes
connected via network and contain traditional multi-core
processors). This work targets, on the other hand, GPU-based systems,
which have different computational model and offer different trade-offs
and challenges.

{\bf Genome Assembly.} Determining the order of nucleotides in a DNA
molecule, a process named sequencing, is an essential process in
biosciences research and discovery. Recently, with the emergence of
new high-throughput sequencing technologies, DNA sequencing machines
have become commodity~\cite{venter2010multiple}, while, at the same
time, offering drastic improvements in sequencing rates: new
sequencing machines, such as the ones offered by 454 life sciences and
Illumina, can generate almost 100 billion nucleotides per day per
machine, which is 50,000 times faster than ten years ago. 

One of the challenges presented by new sequencing technologies is that
they produce much shorter read sequences compared to old sequencing
technologies. Traditional genome assemblers (such as the Celera
assembler) were not designed to work with such data
characteristics. New assemblers such as Velvet~\cite{Zerbino2008} and
Euler~\cite{pevzner2001eulerian} address this challenge by modelling
the problem as constructing and traversing a de Bruijn graph of the
read sequences. However, those tools are sequential and do not scale
in terms of both performance and size (i.e., large mammalian-sized
genomes). 

Simpson et al.~\cite{simpson2009abyss} developed the ABySS parallel
assembler based on MPI. Schatz et al. developed
Contrail~\cite{schatz2010high} which is based on the Hadoop map-reduce
framework. Both this past work, however, assumes traditional homogeneous
cluster based environments.

\section{Methodology}
\label{sec:methodology}
This section describes the steps this project will follow. In summary,
the methodology consists of algorithm design, prototyping, and
performance analysis (both theoretical and experimental).

The first, and ongoing, step is bibliography review in order to better
understand the three areas described in Section~\ref{sec:related}, and
how they relate to each other. Note that the work discussed in Section 
\ref{sec:related} is not an exhaustive list. 

The second phase is to port simple, yet well known graphs 
algorithms to
GPUs~\cite{Meyer2003,Harish2007,Malewicz2009,Sungpack2010}). Implementing
simple graph algorithms on GPUs will give us familiarity with the
algorithms and the platform. Building hands-on expertise on graph
problems is an important step towards designing the framework. It will
inform what should be the building blocks provided by the framework
and how to leverage the platform. Additionally, this step will inform
the theoretical analysis of expected peak performance one can achieve
on particular graph algorithms when executed on GPU platforms.

The third step is to design, implement and analyze an algorithm for the
construction of De Brujin graphs, and computing Eulerian tours on
GPUs. This is a challenging step as the current literature review
already reveals that De Brujin graphs and Eulerian tour paths
algorithms require complex implementation to leverage parallel
architectures~\cite{Quinn1984,Atallah1984}. To complicate matters
further, the parallel architecture assumed in this previous work
does not match current hybrid systems (i.e., shared memory parallel
machines).

Informed by the past steps, the fourth step will focus on defining
building blocks of the graph processing framework, such as the
parallel processing model, abstractions and the high-level
interface. Moreover, this phase aims to define the scope of graph
algorithms the framework will support. In particular, we need to
consider several design trade-offs carefully. Among these design
decisions are: should the framework support directed or undirected
edges? should the programming model supported vertex- or edge-centric?
what is the impact on the class of applications supported? are dynamic
changes allowed in the graph? All these questions will help shaping
the preliminary design of the graph processing framework.

\section{Progress to Date}
\label{sec:progress}

{\em Lauro: I am not sure if we want to follow this structure, but
I am adding what is expected to the mid-term report here. We can change it later.}

Report on progress to date, preliminary findings, risks you have
underestimated, etc.

{\em Matei:  
The only substantial comment relates to approach.  Implementations are costly. So I wonder to what degree the use of a simplified execution model could give you some idea about the bounds of the efficiency one could get on a GPU?   (Or the percent of the bottleneck resource your graph parallel algorithm will be able to use?)  This might help you understand the relative speedups provided by GPUs and CPUs (e.g., compred to, let's say one core), and will force you to think about the critical properties of the algorithms you'll try to implement.   I realize what I am suggesting is, with a relatively high probability, a dead-end.  However, if it does go somewhere it will save a lot of  time.
 
All the rest are minor comments:
 
Pg1: Goals / challenges: I assume there is an implicit question you want to answer.  Can the computational resources offered by hybrid architectures (the combination between a few fast sequential cores and a large number of SIMD cores) be efficiently harnessed by graph algorithms?    Otherwise phrased – given a fixed area or power budget – am I better off with a traditional or with a hybrid architecture?
 
Goals (in section 3): I would deemphasize the framework - this is too ambitious in my opinion.  At this point your goal is a feasibility study.
 
Related work: Google stuff? Map-reduce to process graphs?
 
Would a critical look at past implementations of graph algorithms for GPUs be useful?  I guess this is implicit when you say "The second phase is to port simple, yet well known graphs algorithms to GPUs [5, 8, 9, 14])."
 
Can you provide a clearer delineation for the class of graph problems you are interested in?  By scale? By complexity of algorithm?
}

%% References
\bibliographystyle{abbrv}
\bibliography{graphs}
%\balancecolumns
\end{document}
