\section{Motivation}
\label{sec:motivation}

Graph processing has high memory footprint. Graphs of billions of vertices, which are not unusual ~\cite{graph500}, occupy terabytes of memory space. To support efficient processing of large-scale graphs, expensive supercomputers, such as Cray XMT and BlueGene/P, have been deployed ~\cite{mizell2009early, yoo2005scalable}. Such platforms include large shared-memory space and massive-parallelism to hide memory access latency, a major bottlneck in graph processing. For example, a Cray XT4 supercomputer deployed at the National Energy Research Scientific Computing Center (NERSC) has in total 78TB of shared memory space and 38,288@2.3GHz processor cores with support for hundreds of thousands of threads ~\cite{franklin2011cray}. The machine is able to process graphs as large as 4 billion vertices and 64 billion edges at impressive rates: 5.22 billion edges per second for BFS ~\cite{graph500}. Although these supercomputers can achieve such high processing rates, they are considerably expensive to acquire. For instance, the Cray machine described above costs tens of millions of dollars.

Another common platform to support large-scale graph processing is commodity clusters. Compared to supercomputers, clusters are cheaper to acquire and arguably easier to expand (by simply adding more compute nodes). For example, Malewics et al. ~\cite{Malewicz2009} describes a cluster setup in Google that is based on 300 commodity PCs with over 1TB of aggregate memory space and 1400 processor cores (assuming a PC setup of 4GB of memory and quad-core processor). However, due to high inter-node communication overhead and relatively low multithreading, cluster setups are typically less efficient than supercomputers: roughly 1.3 milllion edges per second of BFS processing rate for a 1 billion vertices and 127 billion edges graph on such setup ~\cite{Malewicz2009}.

Although the rates reported above for each platform are not directly comparable (because of differences in the workloads), one can get the sense that supercomputers are two to three orders of magnitude higher in performance than clusters. To this end, this study explores the opportunity to bring the best of both setups: low-cost and efficiency. The idea is to support cluster setups with massively-parallel commodity components (e.g., GPUs) to improve the performance of graph processing while keeping costs under control.

